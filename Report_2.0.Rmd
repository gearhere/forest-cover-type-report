---
title: "Report about Classification of Forest Cover Type"
date: "August 7, 2019"
output:
  pdf_document: default
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    code_folding: "hide"
    theme: united
    highlight: tango
---

```{r include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

# Introduction

In this report I use the dataset from a kaggle challenge [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction/overview) to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables.

These independent variables were erived from data obtained from the US Geological Survey and USFS.

Six models are involved in this report. They are LDA (Linear Discriminant Analysis), Naive Bayes, kNN (k-NearestNeighbor), Decision Trees, Random Forest and Boosted Trees.

The report can also be used as a step-by-step tutorial for selecting features, cross validation, building models using primary machine learning algorithms and a beginning of kaggle challenges.

---

# Preparation

## Loading libraires

```{r results = 'hide'}
library(ggplot2)
library(dplyr)
library(GGally)
library(Amelia)
library(scales) # Visualization
library(caTools) # Prediction: Splitting Data
library(car) # Prediction: Checking Multicollinearity
library(ROCR) # Prediction: ROC Curve
library(e1071) # Prediction: SVM, Naive Bayes, Parameter Tuning
library(rpart) # Prediction: Decision Tree
library(rpart.plot) # Prediction: Decision Tree
library(randomForest) # Prediction: Random Forest
library(caret) # Prediction: k-Fold Cross Validatio
library(texreg)
library(corrplot)
```

## Data cleaning and exploration

```{r}
# loading the data
train <- read.csv(file.choose())
test <- read.csv(file.choose())
```

```{r}
# creating a new data set including both train and test sets
whole = bind_rows(train, test)
# checking the info about the whole data
str(whole)
```

### Missing Data Imputation

```{r}
# checking missing values
colSums(is.na(whole))
# visualization of missing values
#missmap(whole, main = "Missing values vs observed") running this line will lead to out of memory
```

**Missing values of Covertype all come from the test data so we can just continue without deletion. Hence, it seems that all rows can be used**

**However, some columns have too many 0, which can result in subset full of 0 in the following split. So we still need to find these columns in advance.**

### Further cleaning and reorganization

```{r}
# determing if every value in soil tyoe columns is 0
for (i in which(names(whole) == "Soil_Type1"):55) {print(sum(whole[,i]))}
```

```{r}
# integrating four Wilderness areas into a comprehensive new variable
numArea1 <- which(whole$Wilderness_Area1 == 1)
numArea2 <- which(whole$Wilderness_Area2 == 1)
numArea3 <- which(whole$Wilderness_Area3 == 1)
numArea4 <- which(whole$Wilderness_Area4 == 1)

whole$Wilderness_Area <- "None"

whole[numArea1, which(names(whole) == "Wilderness_Area")] <- 1
whole[numArea2, which(names(whole) == "Wilderness_Area")] <- 2
whole[numArea3, which(names(whole) == "Wilderness_Area")] <- 3
whole[numArea4, which(names(whole) == "Wilderness_Area")] <- 4
```

```{r}
# integrating forty soil types into a comprehensive new variable
numSoil1 <- which(whole$Soil_Type1 == 1)
numSoil2 <- which(whole$Soil_Type2 == 1)
numSoil3 <- which(whole$Soil_Type3 == 1)
numSoil4 <- which(whole$Soil_Type4 == 1)
numSoil5 <- which(whole$Soil_Type5 == 1)
numSoil6 <- which(whole$Soil_Type6 == 1)
numSoil7 <- which(whole$Soil_Type7 == 1)
numSoil8 <- which(whole$Soil_Type8 == 1)
numSoil9 <- which(whole$Soil_Type9 == 1)
numSoil10 <- which(whole$Soil_Type10 == 1)
numSoil11 <- which(whole$Soil_Type11 == 1)
numSoil12 <- which(whole$Soil_Type12 == 1)
numSoil13 <- which(whole$Soil_Type13 == 1)
numSoil14 <- which(whole$Soil_Type14 == 1)
numSoil15 <- which(whole$Soil_Type15 == 1)
numSoil16 <- which(whole$Soil_Type16 == 1)
numSoil17 <- which(whole$Soil_Type17 == 1)
numSoil18 <- which(whole$Soil_Type18 == 1)
numSoil19 <- which(whole$Soil_Type19 == 1)
numSoil20 <- which(whole$Soil_Type20 == 1)
numSoil21 <- which(whole$Soil_Type21 == 1)
numSoil22 <- which(whole$Soil_Type22 == 1)
numSoil23 <- which(whole$Soil_Type23 == 1)
numSoil24 <- which(whole$Soil_Type24 == 1)
numSoil25 <- which(whole$Soil_Type25 == 1)
numSoil26 <- which(whole$Soil_Type26 == 1)
numSoil27 <- which(whole$Soil_Type27 == 1)
numSoil28 <- which(whole$Soil_Type28 == 1)
numSoil29 <- which(whole$Soil_Type29 == 1)
numSoil30 <- which(whole$Soil_Type30 == 1)
numSoil31 <- which(whole$Soil_Type31 == 1)
numSoil32 <- which(whole$Soil_Type32 == 1)
numSoil33 <- which(whole$Soil_Type33 == 1)
numSoil34 <- which(whole$Soil_Type34 == 1)
numSoil35 <- which(whole$Soil_Type35 == 1)
numSoil36 <- which(whole$Soil_Type36 == 1)
numSoil37 <- which(whole$Soil_Type37 == 1)
numSoil38 <- which(whole$Soil_Type38 == 1)
numSoil39 <- which(whole$Soil_Type39 == 1)
numSoil40 <- which(whole$Soil_Type40 == 1)

whole$Soil_Type <- 0

whole[numSoil1, which(names(whole) == "Soil_Type")] <- 1
whole[numSoil2, which(names(whole) == "Soil_Type")] <- 2
whole[numSoil3, which(names(whole) == "Soil_Type")] <- 3
whole[numSoil4, which(names(whole) == "Soil_Type")] <- 4
whole[numSoil5, which(names(whole) == "Soil_Type")] <- 5
whole[numSoil6, which(names(whole) == "Soil_Type")] <- 6
whole[numSoil7, which(names(whole) == "Soil_Type")] <- 7
whole[numSoil8, which(names(whole) == "Soil_Type")] <- 8
whole[numSoil9, which(names(whole) == "Soil_Type")] <- 9
whole[numSoil10, which(names(whole) == "Soil_Type")] <- 10
whole[numSoil11, which(names(whole) == "Soil_Type")] <- 11
whole[numSoil12, which(names(whole) == "Soil_Type")] <- 12
whole[numSoil13, which(names(whole) == "Soil_Type")] <- 13
whole[numSoil14, which(names(whole) == "Soil_Type")] <- 14
whole[numSoil15, which(names(whole) == "Soil_Type")] <- 15
whole[numSoil16, which(names(whole) == "Soil_Type")] <- 16
whole[numSoil17, which(names(whole) == "Soil_Type")] <- 17
whole[numSoil18, which(names(whole) == "Soil_Type")] <- 18
whole[numSoil19, which(names(whole) == "Soil_Type")] <- 19
whole[numSoil20, which(names(whole) == "Soil_Type")] <- 20
whole[numSoil21, which(names(whole) == "Soil_Type")] <- 21
whole[numSoil22, which(names(whole) == "Soil_Type")] <- 22
whole[numSoil23, which(names(whole) == "Soil_Type")] <- 23
whole[numSoil24, which(names(whole) == "Soil_Type")] <- 24
whole[numSoil25, which(names(whole) == "Soil_Type")] <- 25
whole[numSoil26, which(names(whole) == "Soil_Type")] <- 26
whole[numSoil27, which(names(whole) == "Soil_Type")] <- 27
whole[numSoil28, which(names(whole) == "Soil_Type")] <- 28
whole[numSoil29, which(names(whole) == "Soil_Type")] <- 29
whole[numSoil30, which(names(whole) == "Soil_Type")] <- 30
whole[numSoil31, which(names(whole) == "Soil_Type")] <- 31
whole[numSoil32, which(names(whole) == "Soil_Type")] <- 32
whole[numSoil33, which(names(whole) == "Soil_Type")] <- 33
whole[numSoil34, which(names(whole) == "Soil_Type")] <- 34
whole[numSoil35, which(names(whole) == "Soil_Type")] <- 35
whole[numSoil36, which(names(whole) == "Soil_Type")] <- 36
whole[numSoil37, which(names(whole) == "Soil_Type")] <- 37
whole[numSoil38, which(names(whole) == "Soil_Type")] <- 38
whole[numSoil39, which(names(whole) == "Soil_Type")] <- 39
whole[numSoil40, which(names(whole) == "Soil_Type")] <- 40
```

```{r}
# deleting columns of ID、Wilderness_Area1-4、Soil_Type
train_original = whole[1:15120,
                       c(-which(names(whole) =="Id"),
                         -which(names(whole) =="Wilderness_Area1"),
                         -which(names(whole) =="Wilderness_Area2"),
                         -which(names(whole) =="Wilderness_Area3"),
                         -which(names(whole) =="Wilderness_Area4"),
                         -which(names(whole) =="Soil_Type")
                         )]

test_original = whole[15121:581012,
                      c(-which(names(whole) =="Id"),
                        -which(names(whole) =="Wilderness_Area1"),
                        -which(names(whole) =="Wilderness_Area2"),
                        -which(names(whole) =="Wilderness_Area3"),
                        -which(names(whole) =="Wilderness_Area4"),
                        -which(names(whole) =="Soil_Type")
                        )]
```

```{r}
# encoding the categorical features as factors
whole$Wilderness_Area <- factor(whole$Wilderness_Area)
head(whole$Wilderness_Area)
whole$Soil_Type <- factor(whole$Soil_Type)
head(whole$Soil_Type)
```

---

# EDA

```{r results = 'hide'}
str(whole)
```

```{r}
ggplot(filter(whole, is.na(Cover_Type)==FALSE), aes(x=Aspect)) + 
    geom_point(aes(y=Hillshade_9am, color="Hillshade_9am"), alpha=.1) +
    geom_point(aes(y=Hillshade_3pm, color="Hillshade_3pm"), alpha=.1)
```

```{r}
# Exploratory data analysis on relationship be Cover_Type and Wilderness_Area
ggplot(filter(whole, is.na(Cover_Type)==FALSE),
       aes(Wilderness_Area, Cover_Type)
       ) +
  geom_boxplot(aes(col = Wilderness_Area)
               ) +
  theme_bw() +
  ggtitle("Cover_type based on Wilderness_Area")
```

```{r}
#  Exploratory Data Analysis on Cover_Type and Soil_Type
ggplot(filter(whole, is.na(Cover_Type)==FALSE),
       aes(Soil_Type, Cover_Type)
       ) +
  geom_boxplot(aes(col = Soil_Type)) +
  theme_bw() +
  ggtitle("Cover_type based on Soil_Type")
```

**It can be seen that soil type whose index numbers are closing also have similar relationship with cover types.**

```{r}
# checking correlation of numeric variables
train_num = select_if(train, is.numeric)
#correlation matric & shrinking the size of labels
corrplot(cor(train_num),tl.cex=0.5)
```

**Here we find the columns full of 0. We will delete them later.**

---

# Cross Validation

```{r}
# splitting the training set into the training set and validation set
set.seed(789)
split = sample.split(train_original$Cover_Type, SplitRatio = 0.8)
train = subset(train_original, split == TRUE)
validation = subset(train_original, split == FALSE)
```

Another correlation to see the correlation involving categorical variables.
```
library(polycor)
#hetcor(train)
```

```{r}
#cross validation
set.seed(123)
train.control = trainControl(method = "repeatedcv", number =10, repeats=3)
```

```{r results = 'hide'}
# checking if every value in soil tyoe columns is 0 after split
for (i in which(names(train) == "Soil_Type1"):51) {print(sum(train[,i]))}
```

```{r}
# deleting columns full of value 0
train <- train[,c(-which(names(train) == "Soil_Type7"),
                  -which(names(train) == "Soil_Type15")
                  )
               ]
head(train)
```

```{r}
# checking the correlation
train_num = select_if(train, is.numeric)
corrplot(cor(train_num),tl.cex=0.5)
```


---

# t-test

```{r}
t.test(Elevation ~ Soil_Type1,
       mu = 0,
       alt = "two.sided",
       conf = 0.95,
       data=train)
```

**It is self-evident that there is a strong relationship between soil type and elevation.**

---

# Model building

## LDA
```{r results = 'hide'}
# lda model building and CV
model_lda = train(factor(Cover_Type) ~ .,
                  data=train,
                  method="lda",
                  trControl = train.control
                  )
```

```{r}
summary(model_lda)
print(model_lda)
```

## Naive Bayes

**I use smaller subsets of the original dataset or comment out some lines as the output of html is too too too slow. However, I've run a complete model before, so the following comparison is based on that big model.**

```{r}
# make a small subset of train and validation because of the limited computing power
temp_train <- train[1:200,]
validation_temp <- validation[1:50,]
```

```
# naive bayes model building and CV
model_nb = train(factor(Cover_Type) ~ .,
                  data=temp_train,
                  method="nb",
                  #trControl = train.control
                  )
```

## k-NearestNeighbor

```{r}
# Fitting kNN Model to the Training set
classifier_knn <- train(factor(Cover_Type) ~.,
                        data=train,
                        method="knn",
                        trControl=train.control,
                        tuneGrid = expand.grid(k = 1:20) 
                        # can also use tunelength
                        )
```

```{r}
# visualization of kNN model
plot(classifier_knn)
```


## Decision Tree

### Classifier Tree 1
```{r comment=NA}
# Fitting Decision Tree Classification Model to the Training set
classifier_tree <- train(factor(Cover_Type) ~.,
                        data = train,
                        method = "rpart",
                        trControl=train.control,
                        tuneLength = 100
                        )
```

```{r}
# Tree Visualization
plot(classifier_tree$finalModel)
text(classifier_tree$finalModel)
```

```{r}
prp(classifier_tree$finalModel, box.palette = "Reds", tweak = 1.2)
```

```{r}
print(classifier_tree)
```

```{r}
# checking the contribution of each variable
plot(varImp(classifier_tree))
```

### Try to improve the Classifier Tree

```{r}
train.control = trainControl(method = "repeatedcv",
                             number =10,
                             repeats=1
                             )
classifier_tree2 = train(factor(Cover_Type) ~ Elevation *
                            Horizontal_Distance_To_Roadways +
                            Horizontal_Distance_To_Fire_Points +
                            Horizontal_Distance_To_Hydrology +
                            Vertical_Distance_To_Hydrology +
                            Hillshade_9am +
                            Soil_Type3 +
                            Soil_Type10 +
                            Hillshade_Noon +
                            Hillshade_3pm +
                            Soil_Type38 +
                            Soil_Type39 +
                            Soil_Type4 +
                            Soil_Type40 +
                            Soil_Type12 +
                            Soil_Type32 +
                            Soil_Type30 +
                            Soil_Type29 +
                            Wilderness_Area +
                            Horizontal_Distance_To_Hydrology +
                            Aspect,
                        data = train, method = "rpart",
                        trControl=train.control,
                        tuneLength = 100
                        )
```


```{r}
prp(classifier_tree2$finalModel,
    box.palette = "Reds",
    tweak = 1.2
    )
```


```{r}
print(classifier_tree2)
```

## Random forest

```{r}
# Fitting Random Forest Classification Model to the Training set
classifier_forest = train(factor(Cover_Type) ~ Elevation +
                            Horizontal_Distance_To_Roadways +
                            Horizontal_Distance_To_Fire_Points +
                            Horizontal_Distance_To_Hydrology +
                            Hillshade_9am +
                            Soil_Type3 +
                            Soil_Type10,
                        data = train,
                        method = "rf")
                        #trControl=train.control)
```

```{r}
# Forest Visualization
plot(classifier_forest)

set.seed(567)
classifier = randomForest(factor(Cover_Type) ~ Elevation +
                            Horizontal_Distance_To_Roadways +
                            Horizontal_Distance_To_Fire_Points +
                            Horizontal_Distance_To_Hydrology +
                            Hillshade_9am +
                            Soil_Type3 +
                            Soil_Type10,
                          data = train)

# Choosing the number of trees
plot(classifier)
```

**The Random Forest model still has potential as I only choose 7 important features among about 50 variables because of the limited power of CPU. But it still wins the Decision Tree model.**

## Boosted Trees

```{r results='hide'}
# Fitting Boosted Trees Classification Model to the Training set
classifier_btree <- train(factor(Cover_Type) ~ Elevation +
                              Horizontal_Distance_To_Roadways +
                              Horizontal_Distance_To_Fire_Points +
                              Horizontal_Distance_To_Hydrology +
                              Hillshade_9am +
                              Soil_Type3 +
                              Soil_Type10,
                        data = temp_train,
                        method = "gbm",
                        trControl=train.control#,
                        #tuneLength = 100
                        )
```

---

## Prediction

### On validation set

```{r}
# prediction
y_pred_lda = predict(model_lda, newdata = validation)
# Checking the prediction accuracy
table(validation$Cover_Type, y_pred_lda) # Confusion matrix
error_lda <- mean(validation$Cover_Type != y_pred_lda) # Misclassification error
paste('Accuracy',round(1-error_lda,4))
```

```
# prediction
y_pred_nb = predict(model_nb, newdata = validation_temp)
# Checking the prediction accuracy
table(validation_temp$Cover_Type, y_pred_nb) # Confusion matrix
error_nb <- mean(validation_temp$Cover_Type != y_pred_nb) # Misclassification error
paste('Accuracy',round(1-error_nb,4))
```
"Accuracy 0.64"

**Althou in this demo, the NB model seems so-so, the complete model I run before actually gave a very bad prediction about 0.2. One of the reason may be the NB model assumes independence among variables. This apparently conflicts with our data.**

```{r}
# prediction
y_pred_knn = predict(classifier_knn, newdata = validation)
# Checking the prediction accuracy
table(validation$Cover_Type, y_pred_knn) # Confusion matrix
error_knn <- mean(validation$Cover_Type != y_pred_knn) # Misclassification error
paste('Accuracy',round(1-error_knn,4))
```

```{r}
# prediction
y_pred_tree = predict(classifier_tree, newdata = validation)

# Checking the prediction accuracy
table(validation$Cover_Type, y_pred_tree) # Confusion matrix
error_tree <- mean(validation$Cover_Type != y_pred_tree) # Misclassification error
paste('Accuracy',round(1-error_tree,4))
```

```{r}
# prediction of tree2
y_pred_tree2 = predict(classifier_tree2, newdata = validation)

# Checking the prediction accuracy
table(validation$Cover_Type, y_pred_tree2) # Confusion matrix
error_tree2 <- mean(validation$Cover_Type != y_pred_tree2) # Misclassification error
paste('Accuracy',round(1-error_tree2,4))
```

```{r}
# prediction on random forest
y_pred_rf = predict(classifier_forest, newdata = validation)

# Checking the prediction accuracy
table(validation$Cover_Type, y_pred_rf) # Confusion matrix
error_rf <- mean(validation$Cover_Type != y_pred_rf) # Misclassification error
paste('Accuracy',round(1-error_rf,4))
```

```{r}
y_pred_btree = predict(classifier_btree, newdata = validation_temp)

# Checking the prediction accuracy
table(validation_temp$Cover_Type, y_pred_btree) # Confusion matrix
error_btree <- mean(validation_temp$Cover_Type != y_pred_btree) # Misclassification error
paste('Accuracy',round(1-error_btree,4))
```

**Random forest combines the output of multiple decision trees which are somehow "biased" in evaluation. Thus the RF model improves a lot on the foundation of Decision Trees.**

### On test set

**To make the output of html faster, I comment out these lines about writing csv files.**

```
test_original$Cover_Type = predict(model_lda, newdata = test_original)
write.csv(test_original, file = "predicted_lda.csv")
```
#### kaggle score: 0.58346

```
test_original$Cover_Type = predict(model_nb, newdata = test_original)
write.csv(test_original, file = "predicted_nb.csv")
```

```
test_original$Cover_Type = predict(classifier_knn, newdata = test_original)
write.csv(test_original, file = "predicted_knn.csv")
```

#### kaggle score 0.68723

```
test_original$Cover_Type = predict(classifier_tree, newdata = test_original)
write.csv(test_original, file = "predicted_decision_tree.csv")
```

#### kaggle Score 0.63211

```
test_original$Cover_Type = predict(classifier_forest, newdata = test_original)
write.csv(test_original, file = "predicted_random_forest.csv")
```

#### kaggle score 0.6884

```{r comment=NA}
# boosted trees
```

**There is no kaggle score of boosted trees as the above model is just a demo on a limited dataset because of the lack of computing power on the laptop. However, the performance has already been really good on the subset, which is far beyond a random guess among seven types. Thus, we can confidently estimate that the boosted trees model will also give a good performance with the whole training data.**

# Brief Summary

## A Comparison Table of the Quality of Models

|Id|Model|Score|
|--|--|--|
|1|LDA|0.58346|
|2|Naive Bayes|too slow to predict|
|3|kNN|0.68723|
|4|Decision Tree|0.63211|
|5|Random Forest|0.68840|
|6|Bossted Trees|lack|

**To sum up, the Random Forest model is the optimal model among all six models. Although some models can get an accuracy around 0.8 on validation set, they cannot go beyond 0.7 when evaluated by kaggle system. This may due to the much larger size of test dataset than the training dataset.**